{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "342978d6-e16d-4876-8e57-9fb5778080d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflite_runtime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtflite_runtime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtflite\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# import pyttsx3\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tflite_runtime'"
     ]
    }
   ],
   "source": [
    "# The system consists of four main components:\n",
    "# Hand Gesture Recognizer - Uses computer vision to detect and classify hand gestures\n",
    "# Arduino Interface - Communicates with an Arduino microcontroller\n",
    "# Voice Control - Provides speech recognition and text-to-speech capabilities\n",
    "# Main Control System - Integrates all components and manages the overall flow\n",
    "\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['QT_QPA_PLATFORM'] = 'xcb'\n",
    "# Suppress TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy as np\n",
    "import tflite_runtime.interpreter as tflite\n",
    "# import pyttsx3\n",
    "import threading\n",
    "# import speech_recognition as sr\n",
    "from collections import deque\n",
    "import arm_code\n",
    "from picamera2 import Picamera2\n",
    "from libcamera import Transform\n",
    "\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "# ================== CONFIGURATION ==================\n",
    "SIMULATION_MODE = True  # Enable if hardware isn't available\n",
    "FRAME_WIDTH = 1920        # Camera resolution width 640\n",
    "FRAME_HEIGHT = 1080       # Camera resolution height 480\n",
    "CONFIDENCE_THRESHOLD = 0.75  # Minimum confidence for accepting gestures\n",
    "FRAME_SKIP = 2 \n",
    "TARGET_FPS = 15           # Process every nth frame (performance optimization)\n",
    "INVERT_CAMERA = True     # Mirror camera view for more intuitive interaction\n",
    "\n",
    "# ================== HAND GESTURE RECOGNIZER ==================\n",
    "class HandGestureRecognizer:\n",
    "    def __init__(self):\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=1,\n",
    "            model_complexity=1,\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        \n",
    "        self.target_gestures = ['fist', 'rock', 'peace', 'highfive']\n",
    "        self.prediction_queue = deque(maxlen=5)\n",
    "        self.current_gesture = None\n",
    "        self.confidence = 0\n",
    "\n",
    "    def preprocess_landmarks(self, landmarks):\n",
    "        points = np.array([[lm.x, lm.y, lm.z] for lm in landmarks])\n",
    "        \n",
    "        thumb_tip = points[4]\n",
    "        index_tip = points[8]\n",
    "        middle_tip = points[12]\n",
    "        ring_tip = points[16]\n",
    "        pinky_tip = points[20]\n",
    "        \n",
    "        wrist = points[0]\n",
    "        thumb_cmc = points[1]\n",
    "        index_mcp = points[5]\n",
    "        middle_mcp = points[9]\n",
    "        ring_mcp = points[13]\n",
    "        pinky_mcp = points[17]\n",
    "        \n",
    "        thumb_extended = np.linalg.norm(thumb_tip - thumb_cmc) > 0.1\n",
    "        index_extended = np.linalg.norm(index_tip - index_mcp) > 0.1\n",
    "        middle_extended = np.linalg.norm(middle_tip - middle_mcp) > 0.1\n",
    "        ring_extended = np.linalg.norm(ring_tip - ring_mcp) > 0.1\n",
    "        pinky_extended = np.linalg.norm(pinky_tip - pinky_mcp) > 0.1\n",
    "        \n",
    "        if not index_extended and not middle_extended and not ring_extended and not pinky_extended:\n",
    "            return 'fist', 0.9\n",
    "        elif index_extended and not middle_extended and not ring_extended and pinky_extended:\n",
    "            return 'rock', 0.85\n",
    "        elif index_extended and middle_extended and not ring_extended and not pinky_extended:\n",
    "            return 'peace', 0.9\n",
    "        elif index_extended and middle_extended and ring_extended and pinky_extended:\n",
    "            return 'highfive', 0.9\n",
    "        else:\n",
    "            return None, 0\n",
    "\n",
    "    def predict_gesture(self, frame, draw=True):\n",
    "        results = self.hands.process(frame)\n",
    "        gesture = None\n",
    "        confidence = 0\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                if draw:\n",
    "                    self.mp_drawing.draw_landmarks(\n",
    "                        frame,\n",
    "                        hand_landmarks,\n",
    "                        self.mp_hands.HAND_CONNECTIONS\n",
    "                    )\n",
    "                gesture, confidence = self.preprocess_landmarks(hand_landmarks.landmark)\n",
    "                self.prediction_queue.append((gesture, confidence))\n",
    "        else:\n",
    "            self.prediction_queue.clear()\n",
    "            self.current_gesture = None\n",
    "            self.confidence = 0\n",
    "            return None, 0, frame\n",
    "\n",
    "        if len(self.prediction_queue) == self.prediction_queue.maxlen:\n",
    "            gesture_counts = {}\n",
    "            total_confidence = {}\n",
    "            \n",
    "            for g, conf in self.prediction_queue:\n",
    "                if g is not None:\n",
    "                    gesture_counts[g] = gesture_counts.get(g, 0) + 1\n",
    "                    total_confidence[g] = total_confidence.get(g, 0) + conf\n",
    "            \n",
    "            if gesture_counts:\n",
    "                most_common = max(gesture_counts.items(), key=lambda x: x[1])\n",
    "                if most_common[1] >= 3:\n",
    "                    avg_confidence = total_confidence[most_common[0]] / most_common[1]\n",
    "                    if avg_confidence > CONFIDENCE_THRESHOLD:\n",
    "                        self.current_gesture = most_common[0]\n",
    "                        self.confidence = avg_confidence\n",
    "        \n",
    "        return self.current_gesture, self.confidence, frame\n",
    "\n",
    "try:\n",
    "    import arm_code\n",
    "except ImportError:\n",
    "    class arm_code:\n",
    "        @staticmethod\n",
    "        def move(finger, position):\n",
    "            print(f\"[SIMULATION] Moving finger {finger} to position {position}\")\n",
    "            \n",
    "class ASLGestureController:\n",
    "    FRAME_WIDTH = 640\n",
    "    FRAME_HEIGHT = 480\n",
    "    CONFIDENCE_THRESHOLD = 0.85\n",
    "    INVERT_CAMERA = True\n",
    "    FRAME_SKIP = 2\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.last_letter = None\n",
    "        self.last_time = 0\n",
    "        self.asl_letters = ['A', 'B', 'E', 'F', 'H', 'I', 'L', 'W', 'X', 'Y']\n",
    "\n",
    "    def recognize_asl(self, hand_landmarks):\n",
    "        tip_ids = [4, 8, 12, 16, 20]\n",
    "        landmarks = [(lm.x, lm.y) for lm in hand_landmarks.landmark]\n",
    "\n",
    "        fingers = []\n",
    "        thumb_up = landmarks[4][0] < landmarks[3][0]\n",
    "        fingers.append(thumb_up)\n",
    "\n",
    "        for i in range(1, 5):\n",
    "            fingers.append(landmarks[tip_ids[i]][1] < landmarks[tip_ids[i] - 2][1])\n",
    "\n",
    "        if fingers == [True, False, False, False, False]: return \"A\"\n",
    "        elif fingers == [False, True, True, True, True]: return \"B\"\n",
    "\n",
    "        index_down = landmarks[8][1] > landmarks[6][1]\n",
    "        middle_up = landmarks[12][1] < landmarks[10][1]\n",
    "        ring_up = landmarks[16][1] < landmarks[14][1]\n",
    "        pinky_up = landmarks[20][1] < landmarks[18][1]\n",
    "        if index_down and thumb_up and middle_up and ring_up and pinky_up: return \"F\"\n",
    "\n",
    "        if fingers == [False, True, True, False, False]: return \"H\"\n",
    "        if fingers == [False, False, False, False, True]: return \"I\"\n",
    "        if fingers == [True, True, False, False, False]: return \"L\"\n",
    "        if fingers == [False, True, True, True, False]: return \"W\"\n",
    "\n",
    "        index_tip_y, index_dip_y, index_pip_y = landmarks[8][1], landmarks[7][1], landmarks[6][1]\n",
    "        if index_tip_y > index_dip_y and index_tip_y < index_pip_y and not any(fingers[i] for i in [0, 2, 3, 4]):\n",
    "            return \"X\"\n",
    "\n",
    "        if fingers == [True, False, False, False, True]: return \"Y\"\n",
    "\n",
    "        def slightly_bent(tip, pip, mcp):\n",
    "            return landmarks[tip][1] > landmarks[pip][1] and landmarks[tip][1] < landmarks[mcp][1]\n",
    "\n",
    "        if (not fingers[0] and all(slightly_bent(t, t-2, t-3) for t in tip_ids[1:])): return \"E\"\n",
    "\n",
    "        return None\n",
    "\n",
    "    def send_asl_command(self, letter):\n",
    "        print(f\"[ASL] Recognized: {letter}\")\n",
    "        try:\n",
    "            if letter == 'A':\n",
    "                for f in range(5): arm_code.move(f, 0)\n",
    "            elif letter == 'B':\n",
    "                for f in range(1, 5): arm_code.move(f, 2)\n",
    "                arm_code.move(0, 0)\n",
    "            elif letter == 'F':\n",
    "                for f in range(5): arm_code.move(f, 2)\n",
    "                arm_code.move(1, 0)\n",
    "            elif letter == 'H':\n",
    "                arm_code.move(1, 2)\n",
    "                arm_code.move(2, 2)\n",
    "                for f in [0, 3, 4]: arm_code.move(f, 0)\n",
    "            elif letter == 'I':\n",
    "                for f in range(4): arm_code.move(f, 0)\n",
    "                arm_code.move(4, 2)\n",
    "            elif letter == 'L':\n",
    "                arm_code.move(0, 2)\n",
    "                arm_code.move(1, 2)\n",
    "                for f in [2, 3, 4]: arm_code.move(f, 0)\n",
    "            elif letter == 'W':\n",
    "                for f in [1, 2, 3]: arm_code.move(f, 2)\n",
    "                for f in [0, 4]: arm_code.move(f, 0)\n",
    "            elif letter == 'X':\n",
    "                arm_code.move(1, 2)\n",
    "                for f in [0, 2, 3, 4]: arm_code.move(f, 0)\n",
    "            elif letter == 'Y':\n",
    "                arm_code.move(0, 2)\n",
    "                arm_code.move(4, 2)\n",
    "                for f in [1, 2, 3]: arm_code.move(f, 0)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to send ASL command: {e}\")\n",
    "\n",
    "    def run(self):\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.FRAME_WIDTH)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.FRAME_HEIGHT)\n",
    "\n",
    "        with self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.7\n",
    "        ) as hands:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                if self.INVERT_CAMERA:\n",
    "                    frame = cv2.flip(frame, 1)\n",
    "\n",
    "                image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                result = hands.process(image_rgb)\n",
    "\n",
    "                letter = None\n",
    "                if result.multi_hand_landmarks:\n",
    "                    for hand_landmarks in result.multi_hand_landmarks:\n",
    "                        letter = self.recognize_asl(hand_landmarks)\n",
    "                        if letter and (letter != self.last_letter or time.time() - self.last_time > 2):\n",
    "                            self.send_asl_command(letter)\n",
    "                            self.last_letter = letter\n",
    "                            self.last_time = time.time()\n",
    "\n",
    "                        self.mp_drawing.draw_landmarks(frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                if letter:\n",
    "                    cv2.putText(frame, f\"ASL: {letter}\", (10, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                cv2.imshow(\"ASL Gesture Control\", frame)\n",
    "                if cv2.waitKey(1) & 0xFF in [27, ord('q')]:\n",
    "                    break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# ================== MAIN CONTROL SYSTEM ==================\n",
    "class GestureControlSystem:\n",
    "    def __init__(self):\n",
    "        self.running = True\n",
    "        self.frame_count = 0\n",
    "        self.last_gesture_time = 0\n",
    "        self.last_gesture = None\n",
    "        \n",
    "        # Initialize camera with stable configuration\n",
    "        self.picam2 = Picamera2()\n",
    "        config = self.picam2.create_preview_configuration(\n",
    "            main={\"size\": (FRAME_WIDTH, FRAME_HEIGHT), \"format\": \"RGB888\"},\n",
    "            transform=Transform(hflip=INVERT_CAMERA, vflip=False)\n",
    "        )\n",
    "        self.picam2.configure(config)\n",
    "        \n",
    "        # Set camera controls for stable frame rate\n",
    "        self.picam2.set_controls({\n",
    "            \"FrameDurationLimits\": (int(1e6/TARGET_FPS), int(1e6/TARGET_FPS)),\n",
    "            \"AwbEnable\": True,\n",
    "            \"AeEnable\": True,\n",
    "            \"AeExposureMode\": 1,  # Normal exposure\n",
    "            \"AeMeteringMode\": 0,  # Center-weighted\n",
    "            \"NoiseReductionMode\": 2,\n",
    "             \"AwbMode\": 2,  # Grey world white balance\n",
    "            \"ColourGains\": (1.8, 1.2),  # Experiment with values\n",
    "        })\n",
    "        \n",
    "        self.picam2.start()\n",
    "        self.recognizer = HandGestureRecognizer()\n",
    "        self.recognizer = ASLGestureController()\n",
    "\n",
    "        self.fps_queue = deque(maxlen=30)\n",
    "        self.last_time = time.time()\n",
    "\n",
    "    def handle_gesture_command(self, gesture):\n",
    "        responses = {\n",
    "            'fist': (\"Fist bump!\", \"FIST\"),\n",
    "            'rock': (\"Rock on!\", \"ROCK\"),\n",
    "            'peace': (\"Peace!\", \"PEACE\"),\n",
    "            'highfive': (\"High five!\", \"HIGHFIVE\")\n",
    "        }\n",
    "        \n",
    "        current_time = time.time()\n",
    "        if gesture in responses and (gesture != self.last_gesture or current_time - self.last_gesture_time > 2.0):\n",
    "            text, cmd = responses[gesture]\n",
    "            self.send_command(cmd)\n",
    "            self.last_gesture = gesture\n",
    "            self.last_gesture_time = current_time\n",
    "            print(f\"[ACTION] {gesture} detected - Command: {cmd}\")\n",
    "\n",
    "    def send_command(self, command):\n",
    "        print('Sending command to arm:', command)\n",
    "        try:\n",
    "            positions = {\n",
    "                'FIST': [0,0,0,0,0],\n",
    "                'ROCK': [2,2,0,0,2],\n",
    "                'PEACE': [0,2,2,0,0],\n",
    "                'HIGHFIVE': [2,2,2,2,2]\n",
    "            }\n",
    "            \n",
    "            if command in positions:\n",
    "                for finger, pos in enumerate(positions[command]):\n",
    "                    arm_code.move(finger, pos, SIMULATION_MODE)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"[BIONIC ARM HARDWARE] Error sending command: {e}\")\n",
    "            return False\n",
    "\n",
    "    def calculate_fps(self):\n",
    "        current_time = time.time()\n",
    "        fps = 1 / (current_time - self.last_time)\n",
    "        self.last_time = current_time\n",
    "        self.fps_queue.append(fps)\n",
    "        return np.mean(self.fps_queue)\n",
    "\n",
    "    def main_loop(self):\n",
    "        print(\"[SYSTEM] Starting gesture recognition system\")\n",
    "        window_name = 'Hand Gesture Control'\n",
    "        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    \n",
    "        frame_interval = 1.0 / TARGET_FPS\n",
    "        last_frame_time = time.time()\n",
    "    \n",
    "        # MediaPipe Hands setup\n",
    "        mp_hands = mp.solutions.hands\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "        \n",
    "        try:\n",
    "            while self.running:\n",
    "                current_time = time.time()\n",
    "                elapsed = current_time - last_frame_time\n",
    "    \n",
    "                if elapsed < frame_interval:\n",
    "                    time.sleep(frame_interval - elapsed)\n",
    "    \n",
    "                last_frame_time = current_time\n",
    "    \n",
    "                frame = self.picam2.capture_array()\n",
    "    \n",
    "                if INVERT_CAMERA:\n",
    "                    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "                self.frame_count += 1\n",
    "                if self.frame_count % FRAME_SKIP != 0:\n",
    "                    cv2.imshow(window_name, frame)\n",
    "                    if cv2.waitKey(1) & 0xFF in (27, ord('q')):\n",
    "                        self.running = False\n",
    "                    continue\n",
    "    \n",
    "                # ASL recognition logic\n",
    "                image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                result = hands.process(image_rgb)\n",
    "    \n",
    "                letter = None\n",
    "                if result.multi_hand_landmarks:\n",
    "                    for hand_landmarks in result.multi_hand_landmarks:\n",
    "                        # Recognize ASL letter from landmarks\n",
    "                        letter = self.recognize_asl(hand_landmarks)\n",
    "    \n",
    "                        # Only trigger on new gesture or after 2 seconds\n",
    "                        if letter and (letter != self.last_letter or time.time() - self.last_time > 2):\n",
    "                            self.send_asl_command(letter)\n",
    "                            self.last_letter = letter\n",
    "                            self.last_time = time.time()\n",
    "    \n",
    "                        mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "                        cv2.putText(frame, f\"ASL: {letter}\", (10, 30),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "                # Original gesture recognition logic\n",
    "                gesture, confidence, processed_frame = self.recognizer.predict_gesture(frame)\n",
    "    \n",
    "                if gesture and confidence > CONFIDENCE_THRESHOLD:\n",
    "                    self.handle_gesture_command(gesture)\n",
    "    \n",
    "                if gesture:\n",
    "                    cv2.putText(\n",
    "                        processed_frame, \n",
    "                        f\"{gesture} ({confidence:.2f})\", \n",
    "                        (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        1, \n",
    "                        (0, 255, 0) if confidence > CONFIDENCE_THRESHOLD else (0, 0, 255), \n",
    "                        2\n",
    "                    )\n",
    "    \n",
    "                fps = self.calculate_fps()\n",
    "                cv2.putText(\n",
    "                    processed_frame,\n",
    "                    f\"FPS: {fps:.1f}\",\n",
    "                    (FRAME_WIDTH - 120, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7,\n",
    "                    (0, 255, 0),\n",
    "                    2\n",
    "                )\n",
    "    \n",
    "                cv2.imshow(window_name, cv2.cvtColor(processed_frame, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    self.running = False\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Exception in main loop: {e}\")\n",
    "        finally:\n",
    "            self.cleanup()\n",
    "\n",
    "    def cleanup(self):\n",
    "        print(\"[SYSTEM] Cleaning up resources...\")\n",
    "        self.running = False\n",
    "        self.picam2.stop()\n",
    "        cv2.destroyAllWindows()\n",
    "        for _ in range(5):\n",
    "            cv2.waitKey(1)\n",
    "        print(\"[SYSTEM] System shut down\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        system = GestureControlSystem()\n",
    "        system.main_loop()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n[USER] Program stopped by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7260e42-791e-418f-a521-11b2931072b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
